{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "um0w88FNapbn"
      },
      "source": [
        "# **NLP DISASTER TWEET**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ok-jtjV-aO-l",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import re\n",
        "import os\n",
        "import nltk\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4FTvEQra0eR"
      },
      "source": [
        "# **LOAD DATA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwzEis6X22I4",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"/train.csv\")\n",
        "test=pd.read_csv('/test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "60H9JcRwJrgU"
      },
      "outputs": [],
      "source": [
        "# Number of rows and columns\n",
        "num_rows, num_cols = df.shape\n",
        "df.head()\n",
        "df.info()\n",
        "\n",
        "# Data types of each column\n",
        "data_types = df.dtypes\n",
        "\n",
        "# Summary statistics\n",
        "summary_stats = df.describe()\n",
        "\n",
        "# Missing values\n",
        "missing_values = df.isnull().sum()\n",
        "\n",
        "# Unique values in each column\n",
        "unique_values = df.nunique()\n",
        "print(num_rows,num_cols)\n",
        "print('types=',data_types)\n",
        "print('stats=',summary_stats)\n",
        "print('missing values=',missing_values)\n",
        "print('unique_values',unique_values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5-ogIgCpzXQ"
      },
      "source": [
        "# **Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7aVM4N6rp9hn",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Remove URLs and special characters using a for loop\n",
        "cleaned_text_list = []\n",
        "for text in df['text']:\n",
        "    text=text.lower()\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
        "    # Remove \"@\" symbol\n",
        "    text = text.replace('@', '')\n",
        "\n",
        "    # Remove special characters (keep only alphanumeric characters and whitespace)\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
        "    # Uncomment either the stemming or lemmatization section based on your choice\n",
        "    # Stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
        "    cleaned_text_list.append(' '.join(stemmed_tokens))\n",
        "\n",
        "# Add the cleaned_text_list as a new column in the DataFrame\n",
        "df['cleaned_text'] = cleaned_text_list\n",
        "# data split\n",
        "x=df['cleaned_text']\n",
        "y=df['target']\n",
        "x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rupZk8Nho9zi"
      },
      "source": [
        "# **Feature Extraction**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXqFjGX5UvBu",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X= tfidf_vectorizer.fit_transform(df['cleaned_text'])\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1ThKHrcHkCT"
      },
      "source": [
        "# **Classification**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3E3pNx3Hs1v",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "cl_svc=SVC(kernel='linear')\n",
        "cl_svc.fit(X_train, y_train)\n",
        "\n",
        "cl_lr=LogisticRegression()\n",
        "cl_lr.fit(X_train,y_train)\n",
        "\n",
        "cl_knn = KNeighborsClassifier(n_neighbors=2)\n",
        "cl_knn.fit(X_train, y_train)\n",
        "\n",
        "cl_svc_y=cl_svc.predict(X_test)\n",
        "cl_lr_y=cl_lr.predict(X_test)\n",
        "cl_knn_y=cl_knn.predict(X_test)\n",
        "\n",
        "svc_accuracy=accuracy_score(cl_svc_y,y_test)\n",
        "lr_accuracy=accuracy_score(cl_lr_y,y_test)\n",
        "knn_accuracy=accuracy_score(cl_knn_y,y_test)\n",
        "\n",
        "print('svm:',classification_report (y_test,cl_svc_y))\n",
        "print('lr:',classification_report (y_test,cl_lr_y))\n",
        "print('knn',classification_report (y_test,cl_knn_y))\n",
        "\n",
        "train_svm_accuracy=cl_svc.score(X_train, y_train)\n",
        "train_lr_accuracy=cl_lr.score(X_train, y_train)\n",
        "train_knn_accuracy=cl_knn.score(X_train, y_train)\n",
        "\n",
        "print(\"svc_Accuracy:\",svc_accuracy )\n",
        "#print(\"train_svm_Accuracy:\", train_svm_accuracy)\n",
        "print(\"lr_Accuracy:\",lr_accuracy )\n",
        "#print(\"train_lr_Accuracy:\", train_lr_accuracy)\n",
        "print(\"knn_Accuracy:\",knn_accuracy )\n",
        "#print(\"train_knn_Accuracy:\", train_knn_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMjGwrzlmlkL"
      },
      "source": [
        "# **Test Data Preprocessing**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ld39ki4xnPe2",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Remove URLs and special characters using a for loop\n",
        "cleaned_test_list = []\n",
        "for text in test['text']:\n",
        "    text=text.lower()\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
        "\n",
        "    # Remove \"@\" symbol\n",
        "    text = text.replace('@', '')\n",
        "\n",
        "    # Remove special characters (keep only alphanumeric characters and whitespace)\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
        "\n",
        "    # Stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
        "    cleaned_test_list.append(' '.join(stemmed_tokens))\n",
        "\n",
        "# Add the cleaned_test_list as a new column in the DataFrame\n",
        "test['cleaned_text'] = cleaned_test_list\n",
        "test_x = tfidf_vectorizer.transform(test['cleaned_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwQtBmJXvLqu"
      },
      "source": [
        "# **Predicting**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkBNoJz7oJ_9",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "test_pred_svc=cl_svc.predict(test_x)\n",
        "test_pred_lr=cl_lr.predict(test_x)\n",
        "test_pred_knn=cl_knn.predict(test_x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zzfxsp8hJrgW"
      },
      "source": [
        "# **Result**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gozBAX4szIq",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "submissions = pd.read_csv(r\"/sample_submission.csv\")\n",
        "submissions['target']=test_pred_svc\n",
        "submissions.to_csv(\"submissions.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}